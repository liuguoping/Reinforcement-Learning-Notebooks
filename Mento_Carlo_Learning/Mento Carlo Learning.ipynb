{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model-free\n",
    "we do not assume complete knowledge of the environment, or so-called model-free mdp, while dynammic programming methods are only suitable to model-based mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MC methods learn directly from episodes of experience\n",
    "2. MC uses the simplest possible idea: **value = mean return**\n",
    "3. **Caveat**: can only apply MC to episodic MDPs\n",
    "> All episodes **must terminate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mento Carlo Prediction\n",
    " \n",
    "* Goal: learn the state-value function $v(s)$ for a given policy $\\pi$, and a set of episodes obtained by following $\\pi$ and passing through $s$\n",
    "\n",
    "$${ s_1,\\ a_1,\\ r_2,\\ s_2,\\ a_2\\ ....\\ \\sim \\ \\pi}$$\n",
    "\n",
    "\n",
    "* Monte-Carlo policy evaluation uses **empirical mean return** instead of expected return\n",
    "\n",
    "\n",
    "* As more returns are observed, **the average should converge to the expected value**. This idea underlies all Monte Carlo methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-Visit Monte-Carlo Policy Evaluation\n",
    "> Each occurrence of state s in an episode is called a ***visit*** to $s$, Of course, **s may be visited multiple times** in the same episode; let us call the **first time it is visited** in an episode the ***first visit*** to $s$.\n",
    "\n",
    "* The **first-visit** MC method estimates $v(s)$ as the **average of the returns following first visits to s**\n",
    "\n",
    "* The **every-visit** MC method **averages the returns following all visits to s**. \n",
    "\n",
    "######  First-Visit MC Evaluation Algorithm\n",
    "\n",
    "1. Initiate\n",
    "> * $\\pi \\leftarrow $ policy to be evaluated\n",
    "> * $V(s) \\leftarrow $ an arbitrary state-value function\n",
    "> * $Return(s) \\leftarrow $ an empty list, for all s\n",
    "\n",
    "2. Repeated\n",
    "> * Generate an episode using $\\pi$\n",
    "> * For each state s appearing in the episode\n",
    ">> * $G \\leftarrow $return following the first occurrence of s\n",
    ">> * Append G to $Returns(s)$\n",
    ">> * $V(s) \\leftarrow average(\\ Returns(s)\\ )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental Mean\n",
    "\n",
    "The mean $\\mu_1,\\ \\mu_2,\\ ......$ of a sequence $x_1,\\ x_2,\\ ......$ can be computed incrementally,\n",
    "\n",
    "$${\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mu_k &=\\ \\frac{1}{k} \\sum_{i=1}^{k} x_i\\\\\n",
    "&=\\ \\frac{1}{k} (x_k\\ +\\ \\sum_{i=1}^{k-1} x_i)\\\\\n",
    "&=\\ \\frac{1}{k} (x_k\\ +\\ (k-1)\\ \\mu_{k-1})\\\\\n",
    "&=\\ \\mu_{k-1}\\ +\\ \\frac{1}{k} (x_k\\ -\\ \\mu_{k-1})\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental Monte-Carlo Updates\n",
    "\n",
    "Update $V(s)$ incrementally after episode ${ s_1,\\ a_1,\\ r_2,\\ s_2,\\ a_2\\ ....\\ }$\n",
    "\n",
    "1. $N(S)\\ \\leftarrow N(S)\\ +\\ 1$\n",
    "2. $V(S_t)\\ \\leftarrow \\ G_t\\ +\\ \\alpha (G_t\\ -\\ V(S_{t-1}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
