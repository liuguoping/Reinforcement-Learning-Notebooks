{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Property\n",
    "> \"The future is independent of the past given the present\"\n",
    "\n",
    "### Definition\n",
    "\n",
    "A state ${S_t}$ is **Markov** if and only if \n",
    "\n",
    "$${{\\rm P}\\ [\\ S_{t+1}\\ |\\ S_{t}\\ ]\\ =\\ {\\rm P}\\ [\\ S_{t+1}\\ |\\  S_{1},\\ S_{2},..,\\ S_{t}\\ ] }$$\n",
    "\n",
    "### State Transition Matrix\n",
    "\n",
    "For a Markov state ${S}$ and successor state ${S'}$, the **state transition probability** is defined by ${P_{ss'} = {\\rm P}\\ [\\ S_{t+1}\\ =\\ s'\\ |\\ S_{t}\\ =\\ s\\ ] }$ \n",
    "\n",
    "State transition matrix $P$ defines transition probabilities from all states $s$ to all successor states $s'$\n",
    "\n",
    "$${P = \\begin{bmatrix}p_{11} & ... & p_{1n}\\\\ ... & ... & ... \\\\ p_{n1} & ... & p_{nn}\\end{bmatrix} }$$\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Markov Process\n",
    "\n",
    "A Markov Process (or Markov Chain) is a tuple  ${\\big(S,\\ P\\big) }$\n",
    "\n",
    "1. $S$ is a (finite) set of states\n",
    "2. $P$ is a state transition probability matrix\n",
    "\n",
    "#### Example\n",
    "\n",
    "<img src=\"img/mp_example.png\", style=\"width: 500px;\">\n",
    "\n",
    "------\n",
    "\n",
    "## Markov Reward Process\n",
    "\n",
    "A Markov reward process is a **Markov chain with values**, it is a tuple  ${\\big(S,\\ P,\\ R,\\ \\gamma\\big) }$\n",
    "\n",
    "1. $S$ is a (finite) set of states\n",
    "2. $P$ is a state transition probability matrix\n",
    "3. $R$ is a reward function, ${R_s\\ = {\\rm E}\\ [\\ R_{t+1}\\ |\\ S_t\\ =\\ s \\ ]}$\n",
    "4. $\\gamma$ is a discount factor, ${ \\gamma \\in [\\ 0,\\ 1\\ ]}$\n",
    "\n",
    "> - A reward signal defines the goal in a reinforcement learning problem. \n",
    "\n",
    "> - On each time step, the ***environment*** sends to the reinforcement learning agent a single number. \n",
    "\n",
    "> - A Reward is a ***scalar feedback signal***\n",
    "\n",
    "> - Reward signals may be stochastic functions of the state of the environment and the actions taken\n",
    "\n",
    "\n",
    "#### Example\n",
    "<img src=\"img/mrp_example.png\", style=\"width: 500px;\">\n",
    "\n",
    "### Return\n",
    "\n",
    "The return $G_t$ is the **total discounted reward from time-step $t$**\n",
    "\n",
    "$${G_t\\ = \\ R_{t+1}\\ + \\ \\gamma R_{t+2}\\ +\\ ......\\ =\\  \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}}$$\n",
    "\n",
    "> The value of receiving reward $R$ after $k + 1$ time-steps is $\\gamma^k R$\n",
    "\n",
    "### Value Function\n",
    "\n",
    "The state value function $v(s)$ of an MRP is the expected return starting from state $s$\n",
    "\n",
    "$${v(s)\\ = {\\rm E}\\ [\\ G_{t}\\ |\\ S_t\\ =\\ s \\ ]}$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "<img src=\"img/mrp_value_function_example.png\", style=\"width: 500px;\">\n",
    "\n",
    "$${\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v\\ (C3) &= 0.6 * v\\ (Pass) + 0.4 * v\\ (Pub) \\\\\n",
    "&=0.6 * (-2 + 0.9 * 10) + 0.4 * (-2 + 0.9 * 0.8) \\\\\n",
    "&=4.1\n",
    "\\end{aligned}\n",
    "\\end{equation} \n",
    "}$$\n",
    "\n",
    "----\n",
    "\n",
    "## Bellman Equation for MRPs\n",
    "\n",
    "The value function can be decomposed into two parts:\n",
    "1. immediate reward $R_{t+1}$\n",
    "2. discounted value of successor state $\\gamma V(S_{t+1})$\n",
    "\n",
    "$${\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v(s)&= {\\rm E}\\ [\\ G_t\\ |\\ S_t=s\\ ]\\\\\n",
    "&= {\\rm E}\\ [\\ R_{t+1}\\ + \\ \\gamma R_{t+2}\\ + \\ \\gamma^2 R_{t+3}\\ + ... \\ |\\ S_t=s\\ ] \\\\\n",
    "&= {\\rm E}\\ [\\ R_{t+1}\\ + \\ \\gamma (R_{t+2}\\ + \\ \\gamma R_{t+3}\\ + ... )\\ |\\ S_t=s\\ ] \\\\\n",
    "&= {\\rm E}\\ [\\ R_{t+1}\\ + \\ \\gamma G_{t+1}\\ |\\ S_t=s\\ ] \\\\\n",
    "&= {\\rm E}\\ [\\ R_{t+1}\\ + \\ \\gamma v(S_{t+1})\\ |\\ S_t=s\\ ] \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation} \n",
    "}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Diagram\n",
    "\n",
    "$${ v(s)\\ = {\\rm E}\\ [\\ R_{t+1}\\ + \\ \\gamma v(S_{t+1})\\ |\\ S_t=s\\ ]}$$\n",
    "\n",
    "<img src=\"img/mrp_backup.png\",  style=\"width: 400px;\">\n",
    "\n",
    "$${v(s)\\ = \\ R_s\\ + \\gamma \\sum_{s'  \\in S} P_{ss'} v(s')}$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "<img src=\"img/mrp_bellman_example.png\",  style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "for $v\\ (C3)$, here, $\\gamma \\ =\\ 1.0$\n",
    "\n",
    "\n",
    "$${\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v\\ (C3) &= 0.6 * v\\ (Pass) + 0.4 * v\\ (Pub) \\\\\n",
    "&=0.6 * (-2 + 1.0 * 10) + 0.4 * (-2 + 1.0 * 0.8) \\\\\n",
    "&=4.3\n",
    "\\end{aligned}\n",
    "\\end{equation} \n",
    "}$$\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Bellman Equation\n",
    "\n",
    "The Bellman equation is a linear equation\n",
    "\n",
    "$${\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v&=\\ R\\ +\\ \\gamma P v \\\\\n",
    "(I\\ -\\ \\gamma P)\\ v&=\\ R \\\\\n",
    "v& =\\  (I\\ -\\ \\gamma P)^{-1} R\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "}$$\n",
    "\n",
    "1. Computational complexity is very enormous when encountering large states number n\n",
    "2. **Direct solution only possible for small MRPs**\n",
    "3. There are many **iterative methods** for large MRPs, eg: \n",
    "> * Dynamic programming\n",
    "> * Monte-Carlo evaluation\n",
    "> * Temporal-Difference learning\n",
    "> * ......\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of solving Bellman Equation directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Status and its corresponding Rewards\n",
    "states = [\"Class_1\", \"Class_2\", \"Class_3\", \"Facebook\", \"Pub\", \"Pass\", \"Sleep\"]\n",
    "rewards = [-2.0, -2.0, -2.0, -1.0, 1.0 ,10.0, 0.0]\n",
    "\n",
    "states_index = dict(zip(states, range(len(states))))\n",
    "immdiate_reward = dict(zip(states, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class_1': 0,\n",
       " 'Class_2': 1,\n",
       " 'Class_3': 2,\n",
       " 'Facebook': 3,\n",
       " 'Pass': 5,\n",
       " 'Pub': 4,\n",
       " 'Sleep': 6}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class_1': -2.0,\n",
       " 'Class_2': -2.0,\n",
       " 'Class_3': -2.0,\n",
       " 'Facebook': -1.0,\n",
       " 'Pass': 10.0,\n",
       " 'Pub': 1.0,\n",
       " 'Sleep': 0.0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immdiate_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0. ,  0.5,  0. ,  0.5,  0. ,  0. ,  0. ],\n",
       "        [ 0. ,  0. ,  0.8,  0. ,  0. ,  0. ,  0.2],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0.4,  0.6,  0. ],\n",
       "        [ 0.1,  0. ,  0. ,  0.9,  0. ,  0. ,  0. ],\n",
       "        [ 0.2,  0.4,  0.4,  0. ,  0. ,  0. ,  0. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transition Matrix\n",
    "P = np.mat(np.zeros((7, 7)))\n",
    "P[states_index[\"Class_1\"], states_index[\"Class_2\"]] = 0.5\n",
    "P[states_index[\"Class_1\"], states_index[\"Facebook\"]] = 0.5\n",
    "P[states_index[\"Class_2\"], states_index[\"Class_3\"]] = 0.8\n",
    "P[states_index[\"Class_2\"], states_index[\"Sleep\"]] = 0.2\n",
    "P[states_index[\"Class_3\"], states_index[\"Pub\"]] = 0.4\n",
    "P[states_index[\"Class_3\"], states_index[\"Pass\"]] = 0.6\n",
    "P[states_index[\"Facebook\"], states_index[\"Class_1\"]] = 0.1\n",
    "P[states_index[\"Facebook\"], states_index[\"Facebook\"]] = 0.9\n",
    "P[states_index[\"Pub\"], states_index[\"Class_1\"]] = 0.2\n",
    "P[states_index[\"Pub\"], states_index[\"Class_2\"]] = 0.4\n",
    "P[states_index[\"Pub\"], states_index[\"Class_3\"]] = 0.4\n",
    "P[states_index[\"Pass\"], states_index[\"Sleep\"]] = 1.0\n",
    "P[states_index[\"Sleep\"], states_index[\"Sleep\"]] = 1.0\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -2.,  -2.,  -2.,  -1.,   1.,  10.,   0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Immdiate Reward Matrix\n",
    "R = np.mat([\n",
    "    immdiate_reward[\"Class_1\"],\n",
    "    immdiate_reward[\"Class_2\"],\n",
    "    immdiate_reward[\"Class_3\"],\n",
    "    immdiate_reward[\"Facebook\"],\n",
    "    immdiate_reward[\"Pub\"],\n",
    "    immdiate_reward[\"Pass\"],\n",
    "    immdiate_reward[\"Sleep\"],\n",
    "])\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solve the Value Matrix(Vector)\n",
    "# here, set gamma = 0.9\n",
    "gamma = 0.9\n",
    "V = (np.mat(np.eye(7, 7)) - gamma * P).I * R.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> here , **gamma can not be 1.0**, because for aborbing state **Sleep** it satisfied as  $v_{sleep} = 0 + \\gamma * v_{sleep}$. if $\\gamma = 1$, then $v_{sleep} = v_{sleep}$ can not be solved uniquely by matrix solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -5.01272891],\n",
       "        [  0.9426553 ],\n",
       "        [  4.08702125],\n",
       "        [ -7.63760843],\n",
       "        [  1.90839235],\n",
       "        [ 10.        ],\n",
       "        [  0.        ]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class_1': [-5.012728910014522],\n",
       " 'Class_2': [0.9426552976939075],\n",
       " 'Class_3': [4.087021246797094],\n",
       " 'Facebook': [-7.637608431059513],\n",
       " 'Pass': [10.0],\n",
       " 'Pub': [1.9083923522141462],\n",
       " 'Sleep': [0.0]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_value = dict(zip(states, V.tolist()))\n",
    "states_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mrp_value_function_example.png\", style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "A Markov decision process (MDP) is a **Markov reward process with decisions(actions)**. It is an environment in which all states are Markov.\n",
    "\n",
    "it is a tuple  ${\\big(S,\\ A,\\ P,\\ R,\\ \\gamma\\big) }$\n",
    "\n",
    "1. $S$ is a finite set of states\n",
    "2. $A$ is a finite set of actions\n",
    "3. $P$ is a state transition probability matrix, and ${P_{ss'}^{a}\\ = {\\rm P} \\big[\\ S_{t+1}\\ = \\ s'\\ |\\ S_t\\ =\\ s,\\ A_t\\ =\\ a \\big]}$\n",
    "4. $R$ is a reward function, ${R_{s}^{a}\\ = {\\rm E} \\big[\\ R_{t+1}\\ |\\ S_t\\ =\\ s,\\ A_t\\ =\\ a \\big]}$\n",
    "5. $\\gamma$ is a discount factor, ${ \\gamma \\in [\\ 0,\\ 1\\ ]}$\n",
    "\n",
    "\n",
    "<img src=\"img/mdp_concept.png\", style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "A policy $\\pi$ is a distribution over actions given states,\n",
    "\n",
    "$${\\pi(a\\ |\\ s)\\ = {\\rm P} \\ [\\ A_t\\ = a\\ |\\ S_t\\ = s\\ ]}$$\n",
    "\n",
    "1. MDP policies depend on the **current state** (not the history)\n",
    "2. **A policy fully defines the behaviour** of an agent\n",
    "\n",
    "> Given a **MDP**, M =  ${\\big(S,\\ A,\\ P,\\ R,\\ \\gamma\\big) }$, and a **Policy**, $\\pi$\n",
    "\n",
    "> 1. The **state sequence** $S_1,\\ S_2,\\ ......$ is a **Markov process $<S,\\ P^\\pi>$**\n",
    "\n",
    "> 2. The **state and reward sequence**  $S_1,\\ R_1,\\ S_2,\\ R_2,\\ ......$ is a **Markov reward process $<S,\\ P^\\pi,\\ R^\\pi,\\ \\gamma >$** where $${P_{s,s'}^{\\pi}\\ =\\  \\sum_{a \\in A} \\pi (a|s) P_{s,s'}^{a} }$$  and $${R_{s}^{\\pi}\\ =  \\sum_{a \\in A} \\pi (a|s) R_s^a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function\n",
    "\n",
    "#### State-Value function\n",
    "\n",
    "The state-value function $v(s)$ of an MDP is the **expected return starting from state $s$, and then following policy $\\pi$**\n",
    "\n",
    "$${v_{\\pi}(s)\\ = {\\rm E}_{\\pi} [\\ G_t\\ |\\ S_t\\ =\\ s]}$$\n",
    "\n",
    "#### Action-Value function\n",
    "\n",
    "The action-value function $q(s;\\ a)$ is the **expected return starting from state $s$,** ***taking action $a$***, **and then following policy $\\pi$**\n",
    "\n",
    "$${q_{\\pi}(s,\\ a)\\ = {\\rm E}_{\\pi} [\\ G_t\\ |\\ S_t\\ =\\ s,\\ A_t\\ =\\ a]}$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "<img src=\"img/mdp_state_value_example.png\",  style=\"width: 500px;\">\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bellman Expectation Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### state-value function\n",
    "\n",
    "${v_{\\pi}(s)\\ = {\\rm E_{\\pi}} \\ [\\  R_{t+1}\\ +\\ \\gamma v_{\\pi}(S_{t+1})\\ |\\ S_{t}\\ =\\ s \\ ]}$\n",
    "\n",
    "### action-value function\n",
    "\n",
    "${q_{\\pi}(s, a)\\ = {\\rm E_{\\pi}} \\ [\\  R_{t+1}\\ +\\ \\gamma q_{\\pi}(S_{t+1},\\ A_{t+1})\\ |\\ S_{t}\\ =\\ s,\\ A_{t}\\ =\\ a \\ ]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backup diagram for $v(s)$\n",
    "\n",
    "<img src=\"img/bellman_backup_v.png\",  style=\"width: 400px;\">\n",
    "\n",
    "so\n",
    "\n",
    "$${v_{\\pi}(s)\\ =  \\sum_{a \\in A} \\pi (a\\ |\\ s)\\ q_{\\pi}(s,\\ a) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup diagram for $q(s,\\ a)$\n",
    "\n",
    "<img src=\"img/bellman_backup_q.png\",  style=\"width: 400px;\">\n",
    "\n",
    "so\n",
    "\n",
    "$${q_{\\pi}(s,\\ a)\\ = R_{s}^{a}\\ +\\ \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\ v_{\\pi}(s')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup diagram for $v(s)$ again\n",
    "\n",
    "<img src=\"img/bellman_backup_vq.png\",  style=\"width: 400px;\">\n",
    "\n",
    "so\n",
    "\n",
    "$${v_{\\pi}(s)\\ =  \\sum_{a \\in A} \\pi (a\\ |\\ s)\\ \\big(R_{s}^{a}\\ +\\ \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\ v_{\\pi}(s')\\big) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup diagram for $q(s,\\ a)$ again\n",
    "\n",
    "<img src=\"img/bellman_backup_qv.png\",  style=\"width: 400px;\">\n",
    "\n",
    "so\n",
    "\n",
    "$${q_{\\pi}(s,\\ a)\\ = R_{s}^{a}\\ +\\ \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\ \\sum_{a' \\in A} \\pi (a'\\ |\\ s')\\ q_{\\pi}(s',\\ a')}$$\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Value Function\n",
    "\n",
    "The optimal state-value function $v_{*}(s)$ is the maximum value function over all policies\n",
    "\n",
    "$${v_{*}(s)\\ =\\ \\max \\limits_{for\\ all\\ \\pi} v_{\\pi}(s) }$$\n",
    "\n",
    "\n",
    "The optimal action-value function $q_{\\pi}(s,\\ a)$ is the maximum action-value function over all policies\n",
    "\n",
    "$${q_{*}(s,\\ a)\\ =\\ \\max \\limits_{for\\ all\\ \\pi} q_{\\pi}(s,\\ a) }$$\n",
    "\n",
    "> The optimal value function specifies the best possible performance in the MDP\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy\n",
    "\n",
    "### Theorem\n",
    "\n",
    "For any Markov Decision Process:\n",
    "1. There exists an optimal policy $\\pi_{*}$ that is **better than or equal to all other policies**,  $\\pi_{*}\\  \\geq \\ \\pi,\\ \\forall \\pi$\n",
    "2. All optimal policies achieve the **optimal value function**, $v_{\\pi_{*}}(s)\\ =\\ v_{*}(s)$\n",
    "3. All optimal policies achieve the **optimal action-value function**, $q_{\\pi_{*}}(s,\\ a)\\ =\\ q_{*}(s,\\ a)$\n",
    "\n",
    "\n",
    "### Finding an Optimal Policy\n",
    "\n",
    "An optimal policy can be found by maximising over $q_{*}(s,\\ a)$,\n",
    "\n",
    "$${ \\pi_{*}(a\\ |\\ s)\\ =\\begin{cases}1 & if\\ a\\ = \\mathop{\\arg\\max}_{\\forall a}q_{*}(s,\\ a)\\\\0 & otherwise\\end{cases} }$$\n",
    "\n",
    "> There is always **a deterministic optimal policy for any MDP**\n",
    "\n",
    "> If we know $q_{*}(s,\\ a)$, we immediately have the **optimal policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equation\n",
    "\n",
    "### Bellman Optimality Equation for $v_{*}(s)$\n",
    "\n",
    "<img src=\"img/bellman_optimality_v_origin.png\",  style=\"width: 400px;\">\n",
    "\n",
    "so\n",
    "\n",
    "$${v_{*}(s)\\ =\\ \\max \\limits_{a} q_{*}(s,\\ a)  }$$\n",
    "\n",
    "\n",
    "### Bellman Optimality Equation for $q_{*}(s,\\ a)$\n",
    "\n",
    "<img src=\"img/bellman_optimality_q_origin.png\",  style=\"width: 400px;\">\n",
    "\n",
    "so\n",
    "\n",
    "$${q_{*}(s,\\ a)\\ =\\ R_{s}^a\\ + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_{*}(s')    }$$\n",
    "\n",
    "\n",
    "### Bellman Optimality Equation for $v_{*}(s)$ again\n",
    "\n",
    "<img src=\"img/bellman_optimality_v.png\",  style=\"width: 400px;\">\n",
    "\n",
    "so\n",
    "\n",
    "$${v_{*}(s)\\ =\\ \\max \\limits_{a} q_{*}(s,\\ a)\\ =  \\max \\limits_{a} \\ \\big(\\ R_{s}^a\\ + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_{*}(s')\\ \\big)}$$\n",
    "\n",
    "### Bellman Optimality Equation for $q_{*}(s,\\ a)$ again\n",
    "\n",
    "<img src=\"img/bellman_optimality_q.png\",  style=\"width: 400px;\">\n",
    "\n",
    "so\n",
    "\n",
    "$${q_{*}(s,\\ a)\\ =\\ R_{s}^a\\ + \\gamma \\sum_{s' \\in S} P_{ss'}^a \\max \\limits_{a'} q_{*}(s',\\ a')    }$$\n",
    "\n",
    "## Example\n",
    "\n",
    "<img src=\"img/bellman_optimality_example.png\",  style=\"width: 500px;\">\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Bellman Optimality Equation\n",
    "\n",
    "> Bellman Optimality Equation is **non-linear**\n",
    "\n",
    "Many iterative solution methods:\n",
    "1. Value Iteration\n",
    "2. Policy Iteration\n",
    "3. Q-learning\n",
    "4. Sarsa\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
