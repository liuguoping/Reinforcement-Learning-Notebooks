{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming\n",
    "\n",
    "Dynamic Programming is a very general solution method for problems which have **two properties**\n",
    "\n",
    "1. **Optimal substructure**\n",
    "> Optimal solution can be decomposed into subproblems\n",
    "2. **Overlapping subproblems**\n",
    "> * Subproblems recur many times\n",
    "> * Solutions can be cached and reused\n",
    "\n",
    "**Markov decision processes satisfy both properties**\n",
    "\n",
    "* Bellman equation gives recursive decomposition\n",
    "* Value function stores and reuses solutions\n",
    "\n",
    "Dynamic programming assumes **full knowledge of the MDP**, this MDP is so-called **Model-Based**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gridworld_example.png\",  style=\"width: 600px;\">\n",
    "\n",
    "* Undiscounted episodic MDP ( $\\gamma =\\ 1$)\n",
    "* Nonterminal states  $\\ S\\ =\\ \\{1,2,......,14\\}$\n",
    "* Four possible actions  $\\ A\\ =\\ \\{up,\\ down,\\ right,\\ left \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym.envs.toy_text import discrete    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# four action's representation\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "class GridworldEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
    "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
    "    state at the top left or the bottom right corner.\n",
    "\n",
    "    For example, a 4x4 grid looks as follows:\n",
    "\n",
    "    T  o  o  o\n",
    "    o  x  o  o\n",
    "    o  o  o  o\n",
    "    o  o  o  T\n",
    "\n",
    "    x is your position and T are the two terminal states.\n",
    "\n",
    "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
    "    Actions going off the edge leave you in your current state.\n",
    "    You receive a reward of -1 at each step until you reach a terminal state.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape=[4,4]):\n",
    "        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n",
    "            raise ValueError('shape argument must be a list/tuple of length 2')\n",
    "\n",
    "        self.shape = shape\n",
    "        \n",
    "        # nS is the state counts\n",
    "        nS = np.prod(shape)\n",
    "        \n",
    "        # nA is the action counts\n",
    "        nA = 4\n",
    "\n",
    "        MAX_Y = shape[0]\n",
    "        MAX_X = shape[1]\n",
    "\n",
    "        P = {}\n",
    "        grid = np.arange(nS).reshape(shape)\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "        while not it.finished:\n",
    "            s = it.iterindex\n",
    "            y, x = it.multi_index\n",
    "\n",
    "            P[s] = {a : [] for a in range(nA)}\n",
    "\n",
    "            is_done = lambda s: s == 0 or s == (nS - 1)\n",
    "            reward = 0.0 if is_done(s) else -1.0\n",
    "            \n",
    "            # the format of P is (prob, next_state, reward, done)\n",
    "            # here the prob is the probabilty from action to next state\n",
    "            # We're stuck in a terminal state\n",
    "            if is_done(s):\n",
    "                P[s][UP] = [(1.0, s, reward, True)]\n",
    "                P[s][RIGHT] = [(1.0, s, reward, True)]\n",
    "                P[s][DOWN] = [(1.0, s, reward, True)]\n",
    "                P[s][LEFT] = [(1.0, s, reward, True)]\n",
    "            # Not a terminal state\n",
    "            else:\n",
    "                ns_up = s if y == 0 else s - MAX_X\n",
    "                ns_right = s if x == (MAX_X - 1) else s + 1\n",
    "                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n",
    "                ns_left = s if x == 0 else s - 1\n",
    "                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n",
    "                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n",
    "                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n",
    "                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n",
    "\n",
    "            it.iternext()\n",
    "\n",
    "        # Initial state distribution is uniform\n",
    "        # isd is short for initial state distribution\n",
    "        isd = np.ones(nS) / nS\n",
    "\n",
    "        # We expose the model of the environment for educational purposes\n",
    "        # This should not be used in any model-free learning algorithm\n",
    "        self.P = P\n",
    "\n",
    "        super(GridworldEnv, self).__init__(nS, nA, P, isd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation\n",
    "\n",
    "**Problem**: evaluate a given policy $\\pi$, it is also refered to ***prediction problem***\n",
    "\n",
    "**Solution**: iterative application of Bellman expectation backup\n",
    "\n",
    "> Tip: the **existence and uniqueness** of $v_{\\pi}$ are guaranteed **as long as either $\\gamma <\\ 1$ or eventual termination is guranteed** from all states under the policy $\\pi$ \n",
    "\n",
    "#### Alogorithm\n",
    "\n",
    "1. Input $\\pi$, the policy to be evaluated\n",
    "2. Initialize an array $V\\ (s)\\ =\\ 0$, for all $s \\in S$\n",
    "3. Repeat\n",
    "> * ${\\Delta \\leftarrow 0}$\n",
    "> * For each $s \\in S$\n",
    ">> * $v\\ \\leftarrow V(s)$\n",
    ">> * ${V_{\\pi}(s)\\ \\leftarrow  \\sum_{a \\in A} \\pi (a\\ |\\ s)\\ \\big(R_{s}^{a}\\ +\\ \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\ v_{\\pi}(s')\\big) }$\n",
    ">> * $\\Delta \\leftarrow \\max (\\Delta,\\ \\big|\\ v\\ -\\ V(s)\\ \\big|)$\n",
    "4. until $\\Delta\\ <\\ \\theta\\ $ (a small positive number)\n",
    "5. Output $V\\ \\approx v_{\\pi}$\n",
    "  \n",
    "> Tip: the initial approximation, $v_{0}$, **is chosen arbitrarily except that the terminal state, if any, must be given value 0**    why????????????\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "        # print V\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_evaluation(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25],\n",
       "       [ 0.25,  0.25,  0.25,  0.25]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -13.99993529, -19.99990698, -21.99989761,\n",
       "       -13.99993529, -17.9999206 , -19.99991379, -19.99991477,\n",
       "       -19.99990698, -19.99991379, -17.99992725, -13.99994569,\n",
       "       -21.99989761, -19.99991477, -13.99994569,   0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "now we could evaluate value funtion when given a policy, but our **goal is to find better policies**\n",
    "\n",
    "according to ${v_{*}(s)\\ =\\ \\max \\limits_{a} q_{*}(s,\\ a)  }$\n",
    "\n",
    "1. Consider a **deterministic** policy, \n",
    "> $a\\ =\\ \\pi(s)$\n",
    "2. We can improve the policy by acting greedily\n",
    "> $\\pi'(s)\\ = \\mathop{\\arg\\max}_{a\\ \\in \\ A}q_{\\pi}(s,\\ a)$\n",
    "3. This improves the value from **any state s** over one step\n",
    "> $q_{\\pi}(s,\\ \\pi'(s)) = \\max \\limits_{a\\ \\in \\ A}\\ q_{\\pi}(s,\\ a) \\geq q_{\\pi}(s,\\ \\pi(s)) = v_{\\pi}(s)$\n",
    "4. It therefore improves the value function, $v_{\\pi'}(s) \\geq v_{\\pi}(s)$\n",
    "> \n",
    "> $${\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) & \\leq q_{\\pi}(s,\\ \\pi'(s))\\ = {\\rm E_{\\pi'}}[R_{t+1}\\ +\\ \\gamma v_{\\pi}(S_{t+1})\\ |\\ S_t\\ =\\ s)] \\\\\n",
    "& \\leq {\\rm E_{\\pi'}}[R_{t+1}\\ +\\ \\gamma q_{\\pi}(S_{t+1},\\ \\pi'(S_{t+1}))\\ |\\ S_t\\ =\\ s)] \\\\\n",
    "& \\leq {\\rm E_{\\pi'}}[R_{t+1}\\ +\\ \\gamma R_{t+2}\\ + \\gamma^2 q_{\\pi}(S_{t+2},\\ \\pi'(S_{t+2}))\\ |\\ S_t\\ =\\ s)] \\\\\n",
    "& \\leq {\\rm E_{\\pi'}}[R_{t+1}\\ +\\ \\gamma R_{t+2}\\ + \\gamma^2 R_{t+3} +\\ ....... |\\ S_t\\ =\\ s] \\\\\n",
    "& =\\ v_{\\pi'}(s)\n",
    "\\end{aligned}\n",
    "\\end{equation} \n",
    "}$$\n",
    "\n",
    "5. If improvements stop, \n",
    "> $q_{\\pi}(s,\\ \\pi'(s)) = \\max \\limits_{a\\ \\in \\ A}\\ q_{\\pi}(s,\\ a) \\ =\\ q_{\\pi}(s,\\ \\pi(s)) = v_{\\pi}(s)$\n",
    "\n",
    "6. Then the Bellman optimality equation has been satisfied\n",
    "> $v_{\\pi}(s)\\ =\\ \\max \\limits_{a\\ \\in \\ A}\\ q_{\\pi}(s,\\ a) $\n",
    "\n",
    "7. so $\\pi$ is an optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration(using iterative policy evaluation)\n",
    "\n",
    "Problem: find optimal policy $\\pi$\n",
    "\n",
    "#### Alogorithm\n",
    "\n",
    "1. Initialize $V\\ (s)\\ $ and $\\pi (s)$ arbitrarily, for all $s \\in S$\n",
    "2. Repeat\n",
    "> * policy evaluation of $\\pi (s)$, get $V\\ (s)$\n",
    "> * $policy\\_stable \\leftarrow true$\n",
    "> * for each $s \\in S$\n",
    ">> * $old\\_action \\leftarrow \\pi (s)$\n",
    ">> * $\\pi (s) \\leftarrow  \\mathop{\\arg\\max}_{a \\in A} \\big(R_{s}^{a}\\ +\\ \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\ v_{\\pi}(s')\\big) $\n",
    ">> * if $old\\_action \\neq \\pi (s)$,\n",
    ">>> * then $policy\\_stable \\leftarrow false$\n",
    "3. * if $policy\\_stable$, \n",
    "> * then stop and return $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: Lambda discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    epoches = 0\n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # calculate the action value q(s, a)\n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "            # choose a with the max action value q(s, a) as the best a\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        epoches += 1\n",
    "        if policy_stable:\n",
    "            print \"convergence! epoch \", epoches\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence! epoch  3\n",
      "Policy Probability Distribution:\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_improvement(env, policy_evaluation)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "<img src=\"img/policy_iteration.png\",  style=\"width: 500px;\">\n",
    "\n",
    "> Does policy evaluation need to converge to $v_{\\pi}$?\n",
    "\n",
    "> In the small gridworld **k = 3** was sufficient to achieve optimal policy\n",
    "\n",
    "> Why not update policy every iteration?\n",
    "\n",
    "Value iteration can be written as a particularly simple backup operation that **combines the policy improvement and\n",
    "truncated policy evaluation steps**:\n",
    "\n",
    "$v_{k+1}(s)\\ =\\ \\max \\limits_{a} q(s,\\ a)\\ =\\ \\max \\limits_{a} \\big(R_{s}^{a}\\ +\\ \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\ v_{\\pi}(s')\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alogorithm\n",
    "\n",
    "1. Initialize $V\\ (s)\\ $ arbitrarily, for all $s \\in S$\n",
    "2. Initialize an array $V\\ (s)\\ =\\ 0$, for all $s \\in S$\n",
    "3. Repeat\n",
    "> * ${\\Delta \\leftarrow 0}$\n",
    "> * For each $s \\in S$\n",
    ">> * $v\\ \\leftarrow V(s)$\n",
    ">> * ${V_{\\pi}(s)\\ \\leftarrow  \\max \\limits_{a} \\big(R_{s}^{a}\\ +\\ \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\ v_{\\pi}(s')\\big) }$\n",
    ">> * $\\Delta \\leftarrow \\max (\\Delta,\\ \\big|\\ v\\ -\\ V(s)\\ \\big|)$\n",
    "4. until $\\Delta\\ <\\ \\theta\\ $ (a small positive number)\n",
    "5. Output a deterministic policy, $\\pi \\approx \\pi_{*}$, such that \n",
    "> * $\\pi (s)\\ =\\ \\mathop{\\arg\\max}_{a \\in A} \\big(R_{s}^{a}\\ +\\ \\gamma \\sum_{s' \\in S} P_{ss'}^{a}\\ v_{\\pi}(s')\\big)$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment. env.P represents the transition probabilities of the environment.\n",
    "        theta: Stopping threshold. If the value of all states changes less than theta\n",
    "            in one iteration we are done.\n",
    "        discount_factor: lambda time discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function\n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
